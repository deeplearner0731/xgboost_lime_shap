{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f81bdb-6a02-4632-b114-5be84f03d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dde619-0d4c-4bf8-b7ec-cad8ec9df866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############################################################################\n",
    "#  2.  Weight-learning  —  Doubly-robust objective\n",
    "###############################################################################\n",
    "def dr_pseudo_w(y, A, pi, m1, m0):\n",
    "    \"\"\"\n",
    "    DR pseudo-outcome for weight-learning.\n",
    "      c   = (1 - A)/2  +  π*A         ➜  c = π  (treated) , 1-π (control)\n",
    "      φ_DR = ((A01 - π) * (y - m_A)) / c  +  (m1 - m0)\n",
    "    Returns both φ_DR and c (because c is also the weight).\n",
    "    \"\"\"\n",
    "    A01 = (A + 1.0) / 2.0\n",
    "    m_A = np.where(A == 1, m1, m0)\n",
    "    c   = (1.0 - A) / 2.0 + pi * A          # same denominator you used\n",
    "    φ   = (A01 - pi) * (y - m_A) / (c + EPS) + (m1 - m0)\n",
    "    return φ, c + EPS\n",
    "\n",
    "\n",
    "def obj_dr_w(predt, dtrain):\n",
    "    \"\"\"\n",
    "    Custom XGBoost objective for DR weight-learning:\n",
    "      Loss  L = (φ_DR − pred)^2 / c\n",
    "    so   ∂L/∂pred = -2/c (φ_DR − pred)\n",
    "         ∂²L/∂pred² =  2/c\n",
    "    \"\"\"\n",
    "    y = dtrain.get_label()\n",
    "    φ, c = dr_pseudo_w(y, X_train_trt, pi_train, m1_train, m0_train)\n",
    "\n",
    "    grad = -2.0 / c * (φ - predt)\n",
    "    hess =  2.0 / c\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def eval_dr_w(predt, dtrain):\n",
    "    \"\"\"Weighted squared error for monitoring.\"\"\"\n",
    "    y = dtrain.get_label()\n",
    "    φ, c = dr_pseudo_w(y, X_train_trt, pi_train, m1_train, m0_train)\n",
    "    loss = np.mean(((φ - predt) ** 2) / c)\n",
    "    return 'DR_W_loss', float(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c36862d-e2ee-45b3-8cb2-a23837846b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num=random.sample(range(0,1000), 800)\n",
    "test_num=list(set(range(0,1000))-set(train_num))\n",
    "data_simulation = pd.read_csv('/ui/abv/liuzx18/project_shapley/simulation_data/simulation_0_6.csv').iloc[train_num,]\n",
    "data_simulation_test = pd.read_csv('/ui/abv/liuzx18/project_shapley/simulation_data/simulation_0_6.csv').iloc[test_num,]\n",
    "#del data_simulation['Unnamed: 0']\n",
    "#del data_simulation_test['Unnamed: 0']\n",
    "x_df=data_simulation[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']]\n",
    "X_=np.array(x_df)[:]\n",
    "x_df_test=data_simulation_test[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']]\n",
    "X_test=np.array(x_df_test)\n",
    "\n",
    "y_train=np.array(data_simulation[['y']])\n",
    "y_train=y_train.reshape(800,)\n",
    "\n",
    "\n",
    "y_test=np.array(data_simulation_test[['y']])\n",
    "y_test=y_test.reshape(200,)\n",
    "trt_=data_simulation[['treatment']]\n",
    "\n",
    "\n",
    "g_real=data_simulation[['sigpos']]\n",
    "g_real_test=data_simulation_test[['sigpos']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_,trt_)\n",
    "pi_x = logreg.predict_proba(X_)\n",
    "pi_train=pi_x[:,1]\n",
    "\n",
    "\n",
    "\n",
    "X_train_trt=np.where(trt_==1,1,-1)\n",
    "X_train_trt=X_train_trt.reshape(800,)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "x_train_feature_pd=x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c163c1b0-9f90-49ee-8f0f-12f921287ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12    \n",
    "A   = X_train_trt            # ±1 array\n",
    "X   = X_               # feature matrix   (n, p)\n",
    "Y   = y_train                # outcome vector   (n, )\n",
    "K   = 5\n",
    "\n",
    "m1_train = np.zeros_like(Y, dtype=float)\n",
    "m0_train = np.zeros_like(Y, dtype=float)\n",
    "\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=123)\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    # --------- treated arm (+1) ----------\n",
    "    in_arm   = (A[train_idx] ==  1)\n",
    "    booster1 = xgb.XGBRegressor(max_depth=3, n_estimators=200,\n",
    "                                learning_rate=0.05, subsample=0.8,\n",
    "                                colsample_bytree=0.8)\n",
    "    booster1.fit(X[train_idx][in_arm], Y[train_idx][in_arm])\n",
    "    m1_train[test_idx] = booster1.predict(X[test_idx])\n",
    "\n",
    "    # --------- control arm (−1) ----------\n",
    "    in_arm   = (A[train_idx] == -1)\n",
    "    booster0 = xgb.XGBRegressor(max_depth=3, n_estimators=200,\n",
    "                                learning_rate=0.05, subsample=0.8,\n",
    "                                colsample_bytree=0.8)\n",
    "    booster0.fit(X[train_idx][in_arm], Y[train_idx][in_arm])\n",
    "    m0_train[test_idx] = booster0.predict(X[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7129f1c2-7d29-418d-b150-11d387abbb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train AUC: 0.9982020012507816\n",
      "Final Test AUC: 0.9989898989898991\n"
     ]
    }
   ],
   "source": [
    "def dr_pseudo_a(y, A, pi, m1, m0):\n",
    "    \"\"\"\n",
    "    DR pseudo-outcome for A-learning (A = ±1 coding).\n",
    "      φ_DR =  ((A01 - π) * (y - m_A)) / (π * (1 - π))  +  (m1 - m0)\n",
    "    \"\"\"\n",
    "    A01 = (A + 1.0) / 2.0                  # convert ±1 → 0/1\n",
    "    m_A = np.where(A == 1, m1, m0)\n",
    "    denom = pi * (1.0 - pi) + EPS\n",
    "    return (A01 - pi) * (y - m_A) / denom + (m1 - m0)\n",
    "\n",
    "\n",
    "def obj_dr_a(predt, dtrain):\n",
    "    \"\"\"\n",
    "    Custom XGBoost *objective* using the DR pseudo-outcome for A-learning.\n",
    "      Loss  L = (φ_DR − pred)^2     (un-weighted)\n",
    "    \"\"\"\n",
    "    y = dtrain.get_label()\n",
    "    φ = dr_pseudo_a(y, X_train_trt, pi_train, m1_train, m0_train)\n",
    "\n",
    "    grad = -2.0 * (φ - predt)               # ∂L/∂pred\n",
    "    hess =  2.0 * np.ones_like(predt)       # ∂²L/∂pred²\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def eval_dr_a(predt, dtrain):\n",
    "    \"\"\"Squared error on φ_DR (for logging only).\"\"\"\n",
    "    y = dtrain.get_label()\n",
    "    φ = dr_pseudo_a(y, X_train_trt, pi_train, m1_train, m0_train)\n",
    "    loss = np.mean((φ - predt) ** 2)\n",
    "    return 'DR_A_loss', float(loss)\n",
    "\n",
    "best_epoch=1000\n",
    "\n",
    "# --- DR A-learning ----------------------------------------------------------\n",
    "best_params={'learning_rate':0.005,\n",
    "     'verbosity':1,\n",
    "     'booster':'gbtree',\n",
    "     'max_depth':1,\n",
    "     'lambda':5,\n",
    "     'tree_method':'hist' \n",
    "    }\n",
    "\n",
    "model = xgb.train(\n",
    "        best_params,\n",
    "        dtrain,\n",
    "        num_boost_round=best_epoch,  # Use the best num_boost_round\n",
    "        obj=obj_dr_a,\n",
    "        feval=eval_dr_a\n",
    "    )\n",
    "# Predictions and Evaluation on Train Set\n",
    "pred_train = model.predict(dtrain)\n",
    "pred_train_prob = 1.0 / (1.0 + np.exp(-pred_train))  # Sigmoid transformation if needed\n",
    "auc_train = roc_auc_score(g_real.astype(int), pred_train_prob)\n",
    "print(f\"Final Train AUC: {auc_train}\")\n",
    "\n",
    "# Predictions and Evaluation on Test Set\n",
    "pred_test = model.predict(dtest)\n",
    "pred_test_prob = 1.0 / (1.0 + np.exp(-pred_test))  # Sigmoid transformation if needed\n",
    "auc_test = roc_auc_score(g_real_test.astype(int), pred_test_prob)\n",
    "print(f\"Final Test AUC: {auc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0513ede-178e-4683-90dc-e77f81028c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-v2]",
   "language": "python",
   "name": "conda-env-.conda-v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
